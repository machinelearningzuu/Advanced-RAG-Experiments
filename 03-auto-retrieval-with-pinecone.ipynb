{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auto-Retrieval from a Vector Database\n",
    "\n",
    "### Using Metadata for Better Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Many popular vector dbs support a set of `metadata filters` in addition to a query string for semantic search. Given a natural language query, we `first use the LLM to infer a set of metadata filters as well as the right query string` to pass to the vector db (either can also be blank). This overall query bundle is then executed against the vector db.\n",
    "\n",
    "- This allows for more dynamic, expressive forms of retrieval beyond top-k semantic search. The relevant context for a given query may only require filtering on a metadata tag, or require a joint combination of filtering + semantic search within the filtered set, or just raw semantic search.\n",
    "\n",
    "- We demonstrate an example with Pinecone, but auto-retrieval is also implemented with many other vector dbs (e.g. Milviss, Weaviate, and more)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml, os\n",
    "from pinecone import Pinecone\n",
    "from pinecone import ServerlessSpec\n",
    "from llama_index.schema import TextNode\n",
    "from llama_index.llms import AzureOpenAI, OpenAI\n",
    "from llama_index.llm_predictor import LLMPredictor\n",
    "from llama_index import set_global_service_context\n",
    "from llama_index.vector_stores import MetadataFilters\n",
    "from llama_index.embeddings import HuggingFaceEmbedding\n",
    "from llama_index.text_splitter import TokenTextSplitter\n",
    "from llama_index.vector_stores.types import MetadataInfo, VectorStoreInfo\n",
    "from llama_index.vector_stores import PineconeVectorStore, MetadataFilters\n",
    "from llama_index.indices.vector_store.retrievers import VectorIndexAutoRetriever\n",
    "from llama_index import ServiceContext, load_index_from_storage, StorageContext, VectorStoreIndex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure LLMs and VDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cadentials.yaml') as f:\n",
    "    credentials = yaml.load(f, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = credentials[\"PINECONE_API_KEY\"]\n",
    "pc = Pinecone(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_flag = 'DIRECT'\n",
    "\n",
    "embedding_llm = HuggingFaceEmbedding(\n",
    "                                    model_name=\"BAAI/bge-small-en-v1.5\",\n",
    "                                    device='mps'\n",
    "                                    )\n",
    "\n",
    "if llm_flag == 'AZURE':\n",
    "    llm=AzureOpenAI(\n",
    "                    model=credentials['AZURE_ENGINE'],\n",
    "                    api_key=credentials['AZURE_OPENAI_API_KEY'],\n",
    "                    deployment_name=credentials['AZURE_DEPLOYMENT_ID'],\n",
    "                    api_version=credentials['AZURE_OPENAI_API_VERSION'],\n",
    "                    azure_endpoint=credentials['AZURE_OPENAI_API_BASE'],\n",
    "                    temperature=0.3\n",
    "                    )\n",
    "    \n",
    "    chat_llm = LLMPredictor(llm)\n",
    "else:\n",
    "    chat_llm = OpenAI(\n",
    "                    api_key=credentials['DEMO_OPENAI_API_KEY'],\n",
    "                    temperature=0.3\n",
    "                    )\n",
    "\n",
    "text_splitter = TokenTextSplitter(\n",
    "                                separator=\" \",\n",
    "                                chunk_size=1024,\n",
    "                                chunk_overlap=20,\n",
    "                                backup_separators=[\"\\n\"]\n",
    "                                )\n",
    "\n",
    "if llm_flag == 'AZURE':\n",
    "    service_context = ServiceContext.from_defaults(\n",
    "                                                    text_splitter=text_splitter,\n",
    "                                                    # prompt_helper=prompt_helper,\n",
    "                                                    embed_model=embedding_llm,\n",
    "                                                    llm_predictor=chat_llm\n",
    "                                                    )\n",
    "else:\n",
    "    service_context = ServiceContext.from_defaults(\n",
    "                                                    text_splitter=text_splitter,\n",
    "                                                    # prompt_helper=prompt_helper,\n",
    "                                                    embed_model=embedding_llm,\n",
    "                                                    llm=chat_llm\n",
    "                                                    )\n",
    "\n",
    "set_global_service_context(service_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(409)\n",
      "Reason: Conflict\n",
      "HTTP response headers: HTTPHeaderDict({'content-type': 'text/plain; charset=utf-8', 'access-control-allow-origin': '*', 'vary': 'origin,access-control-request-method,access-control-request-headers', 'access-control-expose-headers': '*', 'X-Cloud-Trace-Context': '9ea56095fac1a503258c39cb8b43b398', 'Date': 'Thu, 18 Jan 2024 15:09:57 GMT', 'Server': 'Google Frontend', 'Content-Length': '85', 'Via': '1.1 google', 'Alt-Svc': 'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'})\n",
      "HTTP response body: {\"error\":{\"code\":\"ALREADY_EXISTS\",\"message\":\"Resource  already exists\"},\"status\":409}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Dimensions are for text-embedding-ada-002\n",
    "try:\n",
    "    pc.create_index(\n",
    "                    name=\"advanced-rag-experiments\",\n",
    "                    dimension=384,                  # Replace with your model dimensions\n",
    "                    metric=\"euclidean\",             # Replace with your model metric\n",
    "                    spec=ServerlessSpec(\n",
    "                                        cloud=\"aws\",\n",
    "                                        region=\"us-west-2\"\n",
    "                                        ) \n",
    "                    )\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pinecone_index = pc.Index(\"advanced-rag-experiments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Dummy Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = [\n",
    "        TextNode(\n",
    "                text=\"The Shawshank Redemption\",\n",
    "                metadata={\n",
    "                    \"author\": \"Stephen King\",\n",
    "                    \"theme\": \"Friendship\",\n",
    "                    \"year\": 1994,\n",
    "                },\n",
    "            ),\n",
    "        TextNode(\n",
    "                text=\"The Godfather\",\n",
    "                metadata={\n",
    "                    \"director\": \"Francis Ford Coppola\",\n",
    "                    \"theme\": \"Mafia\",\n",
    "                    \"year\": 1972,\n",
    "                },\n",
    "            ),\n",
    "        TextNode(\n",
    "                text=\"Inception\",\n",
    "                metadata={\n",
    "                    \"director\": \"Christopher Nolan\",\n",
    "                    \"theme\": \"Fiction\",\n",
    "                    \"year\": 2010,\n",
    "                },\n",
    "            ),\n",
    "        TextNode(\n",
    "                text=\"To Kill a Mockingbird\",\n",
    "                metadata={\n",
    "                    \"author\": \"Harper Lee\",\n",
    "                    \"theme\": \"Fiction\",\n",
    "                    \"year\": 1960,\n",
    "                },\n",
    "            ),\n",
    "        TextNode(\n",
    "                text=\"1984\",\n",
    "                metadata={\n",
    "                    \"author\": \"George Orwell\",\n",
    "                    \"theme\": \"Totalitarianism\",\n",
    "                    \"year\": 1949,\n",
    "                },\n",
    "            ),\n",
    "        TextNode(\n",
    "                text=\"The Great Gatsby\",\n",
    "                metadata={\n",
    "                    \"author\": \"F. Scott Fitzgerald\",\n",
    "                    \"theme\": \"The American Dream\",\n",
    "                    \"year\": 1925,\n",
    "                },\n",
    "            ),\n",
    "        TextNode(\n",
    "                text=\"Harry Potter and the Sorcerer's Stone\",\n",
    "                metadata={\n",
    "                    \"author\": \"J.K. Rowling\",\n",
    "                    \"theme\": \"Fiction\",\n",
    "                    \"year\": 1997,\n",
    "                },\n",
    "            ),\n",
    "        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Vector Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = PineconeVectorStore(\n",
    "                                pinecone_index=pinecone_index,\n",
    "                                namespace=\"test\",\n",
    "                                )\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a84413fea01d4b939da53a9d69498ebd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upserted vectors:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "index = VectorStoreIndex(\n",
    "                        nodes, \n",
    "                        storage_context=storage_context\n",
    "                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store_info = VectorStoreInfo(\n",
    "                                content_info=\"famous books and movies\",\n",
    "                                metadata_info=[\n",
    "                                                MetadataInfo(\n",
    "                                                    name=\"director\",\n",
    "                                                    type=\"str\",\n",
    "                                                    description=(\"Name of the director\"),\n",
    "                                                ),\n",
    "                                                MetadataInfo(\n",
    "                                                    name=\"theme\",\n",
    "                                                    type=\"str\",\n",
    "                                                    description=(\"Theme of the book/movie\"),\n",
    "                                                ),\n",
    "                                                MetadataInfo(\n",
    "                                                    name=\"year\",\n",
    "                                                    type=\"int\",\n",
    "                                                    description=(\"Year of the book/movie\"),\n",
    "                                                ),\n",
    "                                            ],\n",
    "                                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = VectorIndexAutoRetriever(\n",
    "                                    index,\n",
    "                                    empty_query_top_k=10,\n",
    "                                    vector_store_info=vector_store_info,\n",
    "                                    default_empty_query_vector=[0] * 384, # this is a hack to allow for blank queries in pinecone\n",
    "                                    verbose=True,\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Querying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Using query str: \n",
      "Using filters: [('year', '>', 2000)]\n",
      "Inception\n",
      "{'director': 'Christopher Nolan', 'theme': 'Fiction', 'year': 2010}\n",
      "Inception\n",
      "{'director': 'Christopher Nolan', 'theme': 'Fiction', 'year': 2010}\n",
      "Inception\n",
      "{'director': 'Christopher Nolan', 'theme': 'Fiction', 'year': 2010}\n",
      "Inception\n",
      "{'director': 'Christopher Nolan', 'theme': 'Fiction', 'year': 2010}\n"
     ]
    }
   ],
   "source": [
    "nodes = retriever.retrieve(\n",
    "    \"Tell me about some books/movies after the year 2000\"\n",
    ")\n",
    "for node in nodes:\n",
    "    print(node.text)\n",
    "    print(node.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using query str: Fiction\n",
      "Using filters: [('theme', '==', 'Fiction')]\n",
      "To Kill a Mockingbird\n",
      "{'author': 'Harper Lee', 'theme': 'Fiction', 'year': 1960}\n",
      "To Kill a Mockingbird\n",
      "{'author': 'Harper Lee', 'theme': 'Fiction', 'year': 1960}\n"
     ]
    }
   ],
   "source": [
    "nodes = retriever.retrieve(\n",
    "    \"Tell me about some books that are Fiction\"\n",
    ")\n",
    "for node in nodes:\n",
    "    print(node.text)\n",
    "    print(node.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pass in Additional Metadata Filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_dicts = [{\n",
    "                \"key\": \"year\", \n",
    "                \"operator\": \"==\", \n",
    "                \"value\": 1997\n",
    "                }]\n",
    "filters = MetadataFilters.from_dicts(filter_dicts)\n",
    "retriever2 = VectorIndexAutoRetriever(\n",
    "                                    index,\n",
    "                                    empty_query_top_k=10,\n",
    "                                    vector_store_info=vector_store_info,\n",
    "                                    default_empty_query_vector=[0] * 384,\n",
    "                                    extra_filters=filters,\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Harry Potter and the Sorcerer's Stone\n",
      "{'author': 'J.K. Rowling', 'theme': 'Fiction', 'year': 1997}\n",
      "Harry Potter and the Sorcerer's Stone\n",
      "{'author': 'J.K. Rowling', 'theme': 'Fiction', 'year': 1997}\n"
     ]
    }
   ],
   "source": [
    "nodes = retriever2.retrieve(\"Tell me about some books that are Fiction\")\n",
    "for node in nodes:\n",
    "    print(node.text)\n",
    "    print(node.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of a failing Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using query str: books\n",
      "Using filters: [('theme', '==', 'mafia'), ('year', '==', 'null'), ('director', '==', 'null')]\n"
     ]
    }
   ],
   "source": [
    "nodes = retriever.retrieve(\"Tell me about some books that are mafia-themed\")\n",
    "for node in nodes:\n",
    "    print(node.text)\n",
    "    print(node.metadata)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llamaindex",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
