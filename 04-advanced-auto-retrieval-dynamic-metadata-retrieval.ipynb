{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml, os\n",
    "from pinecone import Pinecone\n",
    "from pinecone import ServerlessSpec\n",
    "from llama_index.schema import TextNode\n",
    "from llama_index.llms import AzureOpenAI, OpenAI\n",
    "from llama_index.llm_predictor import LLMPredictor\n",
    "from llama_index import set_global_service_context\n",
    "from llama_index.vector_stores import MetadataFilters\n",
    "from llama_index.embeddings import HuggingFaceEmbedding\n",
    "from llama_index.text_splitter import TokenTextSplitter\n",
    "from llama_index.vector_stores.types import MetadataInfo, VectorStoreInfo\n",
    "from llama_index.vector_stores import PineconeVectorStore, MetadataFilters\n",
    "from llama_index.indices.vector_store.retrievers import VectorIndexAutoRetriever\n",
    "from llama_index import ServiceContext, load_index_from_storage, StorageContext, VectorStoreIndex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yet to Complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure LLMs and VDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cadentials.yaml') as f:\n",
    "    credentials = yaml.load(f, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = credentials[\"PINECONE_API_KEY\"]\n",
    "pc = Pinecone(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_flag = 'DIRECT'\n",
    "\n",
    "embedding_llm = HuggingFaceEmbedding(\n",
    "                                    model_name=\"BAAI/bge-small-en-v1.5\",\n",
    "                                    device='mps'\n",
    "                                    )\n",
    "\n",
    "if llm_flag == 'AZURE':\n",
    "    llm=AzureOpenAI(\n",
    "                    model=credentials['AZURE_ENGINE'],\n",
    "                    api_key=credentials['AZURE_OPENAI_API_KEY'],\n",
    "                    deployment_name=credentials['AZURE_DEPLOYMENT_ID'],\n",
    "                    api_version=credentials['AZURE_OPENAI_API_VERSION'],\n",
    "                    azure_endpoint=credentials['AZURE_OPENAI_API_BASE'],\n",
    "                    temperature=0.3\n",
    "                    )\n",
    "    \n",
    "    chat_llm = LLMPredictor(llm)\n",
    "else:\n",
    "    chat_llm = OpenAI(\n",
    "                    api_key=credentials['DEMO_OPENAI_API_KEY'],\n",
    "                    temperature=0.3\n",
    "                    )\n",
    "\n",
    "text_splitter = TokenTextSplitter(\n",
    "                                separator=\" \",\n",
    "                                chunk_size=1024,\n",
    "                                chunk_overlap=20,\n",
    "                                backup_separators=[\"\\n\"]\n",
    "                                )\n",
    "\n",
    "if llm_flag == 'AZURE':\n",
    "    service_context = ServiceContext.from_defaults(\n",
    "                                                    text_splitter=text_splitter,\n",
    "                                                    # prompt_helper=prompt_helper,\n",
    "                                                    embed_model=embedding_llm,\n",
    "                                                    llm_predictor=chat_llm\n",
    "                                                    )\n",
    "else:\n",
    "    service_context = ServiceContext.from_defaults(\n",
    "                                                    text_splitter=text_splitter,\n",
    "                                                    # prompt_helper=prompt_helper,\n",
    "                                                    embed_model=embedding_llm,\n",
    "                                                    llm=chat_llm\n",
    "                                                    )\n",
    "\n",
    "set_global_service_context(service_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(409)\n",
      "Reason: Conflict\n",
      "HTTP response headers: HTTPHeaderDict({'content-type': 'text/plain; charset=utf-8', 'access-control-allow-origin': '*', 'vary': 'origin,access-control-request-method,access-control-request-headers', 'access-control-expose-headers': '*', 'X-Cloud-Trace-Context': '9ea56095fac1a503258c39cb8b43b398', 'Date': 'Thu, 18 Jan 2024 15:09:57 GMT', 'Server': 'Google Frontend', 'Content-Length': '85', 'Via': '1.1 google', 'Alt-Svc': 'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'})\n",
      "HTTP response body: {\"error\":{\"code\":\"ALREADY_EXISTS\",\"message\":\"Resource  already exists\"},\"status\":409}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Dimensions are for text-embedding-ada-002\n",
    "try:\n",
    "    pc.create_index(\n",
    "                    name=\"advanced-rag-experiments\",\n",
    "                    dimension=384,                  # Replace with your model dimensions\n",
    "                    metric=\"euclidean\",             # Replace with your model metric\n",
    "                    spec=ServerlessSpec(\n",
    "                                        cloud=\"aws\",\n",
    "                                        region=\"us-west-2\"\n",
    "                                        ) \n",
    "                    )\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pinecone_index = pc.Index(\"advanced-rag-experiments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/run-llama/llama_index/blob/main/docs/examples/vector_stores/pinecone_auto_retriever.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llamaindex",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
