{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metadata Replacement + Node Sentence Window\n",
    "\n",
    "- similar kind of idealogy to parent-document retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml, os, textwrap, random\n",
    "from llama_index.llms import AzureOpenAI, OpenAI\n",
    "from llama_index.llm_predictor import LLMPredictor\n",
    "from llama_index import set_global_service_context\n",
    "from llama_index.text_splitter import SentenceSplitter\n",
    "from llama_index.embeddings import HuggingFaceEmbedding\n",
    "from llama_index.node_parser import SentenceWindowNodeParser\n",
    "from llama_index.postprocessor import MetadataReplacementPostProcessor\n",
    "from llama_index.evaluation import (\n",
    "                                    DatasetGenerator,\n",
    "                                    QueryResponseDataset,\n",
    "                                    )\n",
    "from llama_index import (\n",
    "                        ServiceContext, \n",
    "                        VectorStoreIndex,\n",
    "                        load_index_from_storage, \n",
    "                        SimpleDirectoryReader, \n",
    "                        StorageContext\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cadentials.yaml') as f:\n",
    "    credentials = yaml.load(f, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_flag = 'DIRECT'\n",
    "\n",
    "embedding_llm = HuggingFaceEmbedding(\n",
    "                                    model_name=\"BAAI/bge-small-en-v1.5\",\n",
    "                                    device='mps'\n",
    "                                    )\n",
    "\n",
    "if llm_flag == 'AZURE':\n",
    "    llm=AzureOpenAI(\n",
    "                    model=credentials['AZURE_ENGINE'],\n",
    "                    api_key=credentials['AZURE_OPENAI_API_KEY'],\n",
    "                    deployment_name=credentials['AZURE_DEPLOYMENT_ID'],\n",
    "                    api_version=credentials['AZURE_OPENAI_API_VERSION'],\n",
    "                    azure_endpoint=credentials['AZURE_OPENAI_API_BASE'],\n",
    "                    temperature=0.3\n",
    "                    )\n",
    "    \n",
    "    chat_llm = LLMPredictor(llm)\n",
    "else:\n",
    "    chat_llm = OpenAI(\n",
    "                    api_key=credentials['DEMO_OPENAI_API_KEY'],\n",
    "                    temperature=0.3\n",
    "                    )\n",
    "\n",
    "if llm_flag == 'AZURE':\n",
    "    service_context = ServiceContext.from_defaults(\n",
    "                                                    embed_model=embedding_llm,\n",
    "                                                    llm_predictor=chat_llm\n",
    "                                                    )\n",
    "else:\n",
    "    service_context = ServiceContext.from_defaults(\n",
    "                                                    embed_model=embedding_llm,\n",
    "                                                    llm=chat_llm\n",
    "                                                    )\n",
    "\n",
    "set_global_service_context(service_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SentenceSplitter\n",
    "- keep sentences and paragraphs together. \n",
    "- Therefore compared to the original TokenTextSplitter, there are `less likely to be hanging sentences or parts of sentences at the end of the node chunk`\n",
    "\n",
    "### SentenceWindowNodeParser\n",
    "- Splits a document into Nodes, with `each node being a sentence` \n",
    "- Each node contains a window from the surrounding sentences in the metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_parser = SentenceWindowNodeParser.from_defaults(\n",
    "                                                    window_size=3,\n",
    "                                                    window_metadata_key=\"window\",\n",
    "                                                    original_text_metadata_key=\"original_text\",\n",
    "                                                    ) # big chunks\n",
    "\n",
    "text_splitter = SentenceSplitter(\n",
    "                                chunk_size=1000,\n",
    "                                chunk_overlap=200\n",
    "                                ) # big chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "172"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = SimpleDirectoryReader(\n",
    "                                input_files=[\"./data/IPCC_AR6_WGII_Chapter03.pdf\"]\n",
    "                                ).load_data()\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_small = node_parser.get_nodes_from_documents(documents)\n",
    "nodes_big = text_splitter.get_nodes_from_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's see how SentenceWindowNodeParser works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of small nodes:  11087\n",
      "number of big nodes:  469\n"
     ]
    }
   ],
   "source": [
    "print(\"number of small nodes: \", len(nodes_small))\n",
    "print(\"number of big nodes: \", len(nodes_big))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contribution of Working Group II to the Sixth Assessment Report of \n",
      "the Intergovernmental Panel on Climate Change [H.-O.   Pörtner, D.C.   Roberts, M.  Tignor, E.S.   Poloczanska, K.  Mintenbeck, \n",
      "A. Alegría, M.  Craig, S.  Langsdorf, S.  Löschke, V .   Möller, A.  Okem, B.  Rama (eds.)].  Cambridge University Press, Cambridge, \n",
      "UK and New York, NY , USA, pp.  \n"
     ]
    }
   ],
   "source": [
    "print(nodes_small[10].metadata['window'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Poloczanska, K.  Mintenbeck, \n",
      "A. Alegría, M.  Craig, S.  Langsdorf, S.  Löschke, V .  \n"
     ]
    }
   ],
   "source": [
    "print(nodes_small[10].metadata['original_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Vector Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_small_index = VectorStoreIndex(\n",
    "                                    nodes_small, \n",
    "                                    service_context=service_context\n",
    "                                    )\n",
    "\n",
    "nodes_big_index = VectorStoreIndex(\n",
    "                                    nodes_big, \n",
    "                                    service_context=service_context\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying\n",
    "\n",
    "- Here, we now use the `MetadataReplacementPostProcessor` to replace the sentence in each node with it’s surrounding context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is low confidence in the quantification of AMOC changes in the 20th century due to disagreement in quantitative reconstructed and\n",
      "simulated trends. Additionally, direct observational records since the mid-2000s are too short to determine the relative contributions of\n",
      "internal variability, natural forcing, and anthropogenic forcing to AMOC change. However, it is highly confident that over the 21st century,\n",
      "AMOC will decline for all SSP scenarios but will not experience an abrupt collapse before 2100.\n"
     ]
    }
   ],
   "source": [
    "query_engine = nodes_small_index.as_query_engine(\n",
    "                                                similarity_top_k=2,\n",
    "                                                node_postprocessors=[\n",
    "                                                                    MetadataReplacementPostProcessor(target_metadata_key=\"window\")\n",
    "                                                                    ]\n",
    "                                                )\n",
    "window_response = query_engine.query(\n",
    "                                    \"What are the concerns surrounding the AMOC?\"\n",
    "                                    )\n",
    "print(textwrap.fill(str(window_response), width=140))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######################## Window ##########################\n",
      "Nevertheless, projected future annual cumulative upwelling wind \n",
      "changes at most locations and seasons remain within ±10–20% of \n",
      "present-day values (medium confidence) (WGI AR6 Section  9.2.3.5; \n",
      "Fox-Kemper et al., 2021).\n",
      " Continuous observation of the Atlantic meridional overturning \n",
      "circulation (AMOC) has improved the understanding of its variability \n",
      "(Frajka-Williams et  al., 2019), but there is low confidence in the \n",
      "quantification of AMOC changes in the 20th century because of low \n",
      "agreement in quantitative reconstructed and simulated trends (WGI \n",
      "AR6 Sections 2.3.3, 9.2.3.1; Fox-Kemper et al., 2021; Gulev et al., 2021). \n",
      " Direct observational records since the mid-2000s remain too short to \n",
      "determine the relative contributions of internal variability, natural \n",
      "forcing and anthropogenic forcing to AMOC change (high confidence) \n",
      "(WGI AR6 Sections 2.3.3, 9.2.3.1; Fox-Kemper et al., 2021; Gulev et al., \n",
      "2021).  Over the 21st century, AMOC will very likely decline for all SSP \n",
      "scenarios but will not involve an abrupt collapse before 2100 (WGI \n",
      "AR6 Sections 4.3.2, 9.2.3.1; Fox-Kemper et al., 2021; Lee et al., 2021).\n",
      " 3.2.2.4 Sea Ice Changes\n",
      "Sea ice is a key driver of polar marine life, hosting unique ecosystems \n",
      "and affecting diverse marine organisms and food webs through its \n",
      "impact on light penetration and supplies of nutrients and organic \n",
      "matter (Arrigo, 2014).  Since the late 1970s, Arctic sea ice area has \n",
      "decreased for all months, with an estimated decrease of 2 million km2 \n",
      "(or 25%) for summer sea ice (averaged for August, September and \n",
      "October) in 2010–2019 as compared with 1979–1988 (WGI AR6 \n",
      "Section 9.3.1.1; Fox-Kemper et al., 2021). \n",
      "\n",
      "\n",
      "\n",
      "################### Original Sentence ####################\n",
      "Over the 21st century, AMOC will very likely decline for all SSP \n",
      "scenarios but will not involve an abrupt collapse before 2100 (WGI \n",
      "AR6 Sections 4.3.2, 9.2.3.1; Fox-Kemper et al., 2021; Lee et al., 2021).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "window = window_response.source_nodes[0].node.metadata[\"window\"]\n",
    "sentence = window_response.source_nodes[0].node.metadata[\"original_text\"]\n",
    "\n",
    "print(f\"######################## Window ##########################\\n{window}\")\n",
    "print('\\n\\n')\n",
    "print(f\"################### Original Sentence ####################\\n{sentence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are concerns surrounding the Atlantic overturning circulation (AMOC). There is low confidence in reconstructed and modelled AMOC\n",
      "changes for the 20th century. However, it is projected that the AMOC will decline over the 21st century with high confidence, although there\n",
      "is low confidence for quantitative projections.\n"
     ]
    }
   ],
   "source": [
    "query_engine_big = nodes_big_index.as_query_engine(similarity_top_k=2)\n",
    "vector_response = query_engine_big.query(\n",
    "    \"What are the concerns surrounding the AMOC?\"\n",
    "    )\n",
    "print(textwrap.fill(str(vector_response), width=140))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why `SentenceWindowNodeParser + MetadataReplacementNodePostProcessor` is the Winner here ?\n",
    "- Embeddings at a sentence level seem to capture more fine-grained details, like the word `AMOC`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes_eval = 30 #there are 469 big nodes total. Take the first 200 to generate questions (the back half of the doc is all references)\n",
    "sample_eval_nodes = random.sample(nodes_big[:200], num_nodes_eval) # NOTE: run this if the dataset isn't already saved\n",
    "eval_service_context = ServiceContext.from_defaults(llm=chat_llm)\n",
    "dataset_generator = DatasetGenerator(\n",
    "                                    sample_eval_nodes,\n",
    "                                    service_context=eval_service_context,\n",
    "                                    num_questions_per_chunk=2,\n",
    "                                    show_progress=True\n",
    "                                    ) # generate questions from the largest chunks (1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset = await dataset_generator.agenerate_dataset_from_nodes()\n",
    "eval_dataset.save_json(\"generated/ipcc_eval_qr_dataset.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset = QueryResponseDataset.from_json(\"data/ipcc_eval_qr_dataset.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llamaindex",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
