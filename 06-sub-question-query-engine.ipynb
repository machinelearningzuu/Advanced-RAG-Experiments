{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sub Question Query Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In this tutorial, we showcase how to use a sub question query engine to tackle the problem of `answering a complex query using multiple data sources.`\n",
    "\n",
    "- It first breaks down the `complex query into sub questions` for each relevant data source, then gather all the intermediate reponses and synthesizes a final response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml, logging\n",
    "from llama_index.llms import AzureOpenAI, OpenAI\n",
    "from llama_index.llm_predictor import LLMPredictor\n",
    "from llama_index.text_splitter import TokenTextSplitter\n",
    "from llama_index.embeddings import HuggingFaceEmbedding\n",
    "from llama_index.tools import QueryEngineTool, ToolMetadata\n",
    "from llama_index.query_engine import SubQuestionQueryEngine\n",
    "from llama_index.callbacks.schema import CBEventType, EventPayload\n",
    "from llama_index.callbacks import CallbackManager, LlamaDebugHandler\n",
    "from llama_index import (\n",
    "                        VectorStoreIndex, \n",
    "                        SimpleDirectoryReader, \n",
    "                        set_global_service_context,\n",
    "                        ServiceContext\n",
    "                        )\n",
    "\n",
    "logging.getLogger(\"transformers.tokenization_utils_base\").setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cadentials.yaml') as f:\n",
    "    credentials = yaml.load(f, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_flag = 'DIRECT'\n",
    "\n",
    "embedding_llm = HuggingFaceEmbedding(\n",
    "                                    model_name=\"BAAI/bge-small-en-v1.5\",\n",
    "                                    device='mps'\n",
    "                                    )\n",
    "\n",
    "if llm_flag == 'AZURE':\n",
    "    llm=AzureOpenAI(\n",
    "                    model=credentials['AZURE_ENGINE'],\n",
    "                    api_key=credentials['AZURE_OPENAI_API_KEY'],\n",
    "                    deployment_name=credentials['AZURE_DEPLOYMENT_ID'],\n",
    "                    api_version=credentials['AZURE_OPENAI_API_VERSION'],\n",
    "                    azure_endpoint=credentials['AZURE_OPENAI_API_BASE'],\n",
    "                    temperature=0.3\n",
    "                    )\n",
    "    \n",
    "    chat_llm = LLMPredictor(llm)\n",
    "else:\n",
    "    chat_llm = OpenAI(\n",
    "                    api_key=credentials['OPENAI_API_KEY'],\n",
    "                    temperature=0.3\n",
    "                    )\n",
    "\n",
    "text_splitter = TokenTextSplitter(\n",
    "                                separator=\" \",\n",
    "                                chunk_size=1024,\n",
    "                                chunk_overlap=20,\n",
    "                                backup_separators=[\"\\n\"]\n",
    "                                )\n",
    "\n",
    "# Using the LlamaDebugHandler to print the trace of the sub questions captured by the SUB_QUESTION callback event type\n",
    "llama_debug = LlamaDebugHandler(print_trace_on_end=True)\n",
    "callback_manager = CallbackManager([llama_debug])\n",
    "\n",
    "if llm_flag == 'AZURE':\n",
    "    service_context = ServiceContext.from_defaults(\n",
    "                                                    text_splitter=text_splitter,\n",
    "                                                    callback_manager=callback_manager,\n",
    "                                                    embed_model=embedding_llm,\n",
    "                                                    llm_predictor=chat_llm\n",
    "                                                    )\n",
    "else:\n",
    "    service_context = ServiceContext.from_defaults(\n",
    "                                                    text_splitter=text_splitter,\n",
    "                                                    callback_manager=callback_manager,\n",
    "                                                    embed_model=embedding_llm,\n",
    "                                                    llm=chat_llm\n",
    "                                                    )\n",
    "\n",
    "set_global_service_context(service_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-01-26 08:59:16--  https://raw.githubusercontent.com/run-llama/llama_index/main/docs/examples/data/paul_graham/paul_graham_essay.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.111.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 75042 (73K) [text/plain]\n",
      "Saving to: ‘data/paul_graham/paul_graham_essay.txt’\n",
      "\n",
      "data/paul_graham/pa 100%[===================>]  73.28K   400KB/s    in 0.2s    \n",
      "\n",
      "2024-01-26 08:59:17 (400 KB/s) - ‘data/paul_graham/paul_graham_essay.txt’ saved [75042/75042]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p 'data/paul_graham/'\n",
    "!wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/examples/data/paul_graham/paul_graham_essay.txt' -O 'data/paul_graham/paul_graham_essay.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\n",
      "Trace: index_construction\n",
      "    |_embedding ->  0.610594 seconds\n",
      "    |_embedding ->  0.610578 seconds\n",
      "**********\n"
     ]
    }
   ],
   "source": [
    "pg_essay = SimpleDirectoryReader(input_dir=\"./data/paul_graham/\").load_data()\n",
    "\n",
    "# build index and query engine\n",
    "vector_index = VectorStoreIndex.from_documents(\n",
    "                                            pg_essay, \n",
    "                                            use_async=True, \n",
    "                                            service_context=service_context\n",
    "                                            )\n",
    "vector_query_engine = vector_index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine_tools = [\n",
    "                        QueryEngineTool(\n",
    "                            query_engine=vector_query_engine,\n",
    "                            metadata=ToolMetadata(\n",
    "                                                name=\"pg_essay\",\n",
    "                                                description=\"Paul Graham essay on What I Worked On\",\n",
    "                                                ),\n",
    "                                        ),\n",
    "                    ]\n",
    "\n",
    "query_engine = SubQuestionQueryEngine.from_defaults(\n",
    "                                                    query_engine_tools=query_engine_tools,\n",
    "                                                    service_context=service_context,\n",
    "                                                    use_async=True,\n",
    "                                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 3 sub questions.\n",
      "\u001b[1;3;38;2;237;90;200m[pg_essay] Q: What did Paul Graham work on before YC?\n",
      "\u001b[0m\u001b[1;3;38;2;90;149;237m[pg_essay] Q: What did Paul Graham work on during YC?\n",
      "\u001b[0m\u001b[1;3;38;2;11;159;203m[pg_essay] Q: What did Paul Graham work on after YC?\n",
      "\u001b[0m\u001b[1;3;38;2;11;159;203m[pg_essay] A: After Y Combinator, Paul Graham worked on painting and writing essays. He spent most of 2014 painting and then started writing essays again, including some that were not about startups.\n",
      "\u001b[0m\u001b[1;3;38;2;90;149;237m[pg_essay] A: During his time at Y Combinator (YC), Paul Graham worked on various tasks and responsibilities. However, the specific details of what he worked on during YC are not mentioned in the given context.\n",
      "\u001b[0m\u001b[1;3;38;2;237;90;200m[pg_essay] A: Before Y Combinator (YC), Paul Graham worked on writing and programming. He wrote short stories and also wrote programs on the IBM 1401 computer in his school's basement. He later got a microcomputer, a TRS-80, and started programming more extensively, creating simple games, a rocket prediction program, and a word processor.\n",
      "\u001b[0m**********\n",
      "Trace: query\n",
      "    |_query ->  15.24709 seconds\n",
      "      |_llm ->  5.545841 seconds\n",
      "      |_sub_question ->  4.424369 seconds\n",
      "        |_query ->  4.424001 seconds\n",
      "          |_retrieve ->  0.035513 seconds\n",
      "            |_embedding ->  0.034494 seconds\n",
      "          |_synthesize ->  4.388304 seconds\n",
      "            |_templating ->  1.4e-05 seconds\n",
      "            |_llm ->  4.381277 seconds\n",
      "      |_sub_question ->  3.403864 seconds\n",
      "        |_query ->  3.403538 seconds\n",
      "          |_retrieve ->  0.024715 seconds\n",
      "            |_embedding ->  0.023863 seconds\n",
      "          |_synthesize ->  3.378709 seconds\n",
      "            |_templating ->  1.8e-05 seconds\n",
      "            |_llm ->  3.373102 seconds\n",
      "      |_sub_question ->  2.764131 seconds\n",
      "        |_query ->  2.763812 seconds\n",
      "          |_retrieve ->  0.021418 seconds\n",
      "            |_embedding ->  0.020638 seconds\n",
      "          |_synthesize ->  2.7423 seconds\n",
      "            |_templating ->  1.2e-05 seconds\n",
      "            |_llm ->  2.736449 seconds\n",
      "      |_synthesize ->  5.274672 seconds\n",
      "        |_templating ->  3.3e-05 seconds\n",
      "        |_llm ->  5.271274 seconds\n",
      "**********\n",
      "Paul Graham's life was different before, during, and after Y Combinator (YC). Before YC, he focused on writing short stories and programming on computers such as the IBM 1401 and TRS-80. During his time at YC, the specific details of his work are not mentioned. After YC, Graham shifted his focus to painting and writing essays, including some that were not related to startups.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\n",
    "                            \"How was Paul Grahams life different before, during, and after YC?\"\n",
    "                            )\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub Question 0: What did Paul Graham work on before YC?\n",
      "Answer: Before Y Combinator (YC), Paul Graham worked on writing and programming. He wrote short stories as a beginning writer and also tried programming on the IBM 1401 computer in his school district. He later got a microcomputer, a TRS-80, and started programming more extensively, creating simple games, a rocket prediction program, and a word processor.\n",
      "====================================\n",
      "Sub Question 1: What did Paul Graham work on during YC?\n",
      "Answer: During his time at Y Combinator (YC), Paul Graham worked on various projects and responsibilities. However, the specific details of what he worked on during YC are not mentioned in the given context.\n",
      "====================================\n",
      "Sub Question 2: What did Paul Graham work on after YC?\n",
      "Answer: After Y Combinator, Paul Graham worked on painting and writing essays.\n",
      "====================================\n",
      "Sub Question 3: What did Paul Graham work on before YC?\n",
      "Answer: Before Y Combinator (YC), Paul Graham worked on writing and programming. He wrote short stories and also wrote programs on the IBM 1401 computer in his school's basement. He later got a microcomputer, a TRS-80, and started programming more extensively, creating simple games, a rocket prediction program, and a word processor.\n",
      "====================================\n",
      "Sub Question 4: What did Paul Graham work on during YC?\n",
      "Answer: During his time at Y Combinator (YC), Paul Graham worked on various tasks and responsibilities. However, the specific details of what he worked on during YC are not mentioned in the given context.\n",
      "====================================\n",
      "Sub Question 5: What did Paul Graham work on after YC?\n",
      "Answer: After Y Combinator, Paul Graham worked on painting and writing essays. He spent most of 2014 painting and then started writing essays again, including some that were not about startups.\n",
      "====================================\n"
     ]
    }
   ],
   "source": [
    "for i, (start_event, end_event) in enumerate(\n",
    "                                            llama_debug.get_event_pairs(CBEventType.SUB_QUESTION)\n",
    "                                            ):\n",
    "    qa_pair = end_event.payload[EventPayload.SUB_QUESTION]\n",
    "    print(\"Sub Question \" + str(i) + \": \" + qa_pair.sub_q.sub_question.strip())\n",
    "    print(\"Answer: \" + qa_pair.answer.strip())\n",
    "    print(\"====================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llamaindex",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
